{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled30.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurvaChiniya/Sentiment_analysis/blob/master/LSTM_TEXT_CLASSIFIER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4dvmkvnpF_",
        "colab_type": "text"
      },
      "source": [
        "# IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5pjds9Rua0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lRs2DGDn9NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzW2PU0EwRL5",
        "colab_type": "text"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "184KJsbXsr3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4dcb7eb1-d20d-4afe-ba80-81d7c98b40f4"
      },
      "source": [
        "tokenize = lambda x: x.split()\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "LABEL = data.LabelField()\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:10<00:00, 7.95MB/s]\n",
            ".vector_cache/glove.6B.zip: 862MB [00:54, 15.9MB/s]                           \n",
            "100%|█████████▉| 399864/400000 [00:37<00:00, 10726.72it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsqA5dU4sudg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lydqYgEs2-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6ff69c6b-1e86-4a56-d1eb-2409d138b0b4"
      },
      "source": [
        "print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "print (\"Label Length: \" + str(len(LABEL.vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 251639\n",
            "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
            "Label Length: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMAmN5A_tD0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split()\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZEV518CtlUU",
        "colab_type": "text"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBVzZzRmt2t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfjNpxQotnA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z2kCn27tqw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J81Ayw1jtu2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSivbbe_wFtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3l5mh8rwGLk",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnKi_M7EtxtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 2e-5\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6pGyMnGuPCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76130f30-9de3-437b-9c55-074b4bb3f269"
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Idx: 100, Training Loss: 0.6876, Training Accuracy:  53.12%\n",
            "Epoch: 1, Idx: 200, Training Loss: 0.6988, Training Accuracy:  53.12%\n",
            "Epoch: 1, Idx: 300, Training Loss: 0.6849, Training Accuracy:  53.12%\n",
            "Epoch: 1, Idx: 400, Training Loss: 0.6799, Training Accuracy:  59.38%\n",
            "Epoch: 1, Idx: 500, Training Loss: 0.6766, Training Accuracy:  65.62%\n",
            "Epoch: 01, Train Loss: 0.686, Train Acc: 53.90%, Val. Loss: 0.678047, Val. Acc: 54.43%\n",
            "Epoch: 2, Idx: 100, Training Loss: 0.6078, Training Accuracy:  71.88%\n",
            "Epoch: 2, Idx: 200, Training Loss: 0.6427, Training Accuracy:  65.62%\n",
            "Epoch: 2, Idx: 300, Training Loss: 0.6748, Training Accuracy:  59.38%\n",
            "Epoch: 2, Idx: 400, Training Loss: 0.6975, Training Accuracy:  56.25%\n",
            "Epoch: 2, Idx: 500, Training Loss: 0.5721, Training Accuracy:  81.25%\n",
            "Epoch: 02, Train Loss: 0.657, Train Acc: 61.81%, Val. Loss: 0.642345, Val. Acc: 62.57%\n",
            "Epoch: 3, Idx: 100, Training Loss: 0.7290, Training Accuracy:  46.88%\n",
            "Epoch: 3, Idx: 200, Training Loss: 0.7403, Training Accuracy:  53.12%\n",
            "Epoch: 3, Idx: 300, Training Loss: 0.6604, Training Accuracy:  59.38%\n",
            "Epoch: 3, Idx: 400, Training Loss: 0.7329, Training Accuracy:  59.38%\n",
            "Epoch: 3, Idx: 500, Training Loss: 0.6235, Training Accuracy:  68.75%\n",
            "Epoch: 03, Train Loss: 0.629, Train Acc: 65.62%, Val. Loss: 0.635494, Val. Acc: 64.00%\n",
            "Epoch: 4, Idx: 100, Training Loss: 0.6635, Training Accuracy:  56.25%\n",
            "Epoch: 4, Idx: 200, Training Loss: 0.6115, Training Accuracy:  71.88%\n",
            "Epoch: 4, Idx: 300, Training Loss: 0.7045, Training Accuracy:  53.12%\n",
            "Epoch: 4, Idx: 400, Training Loss: 0.6359, Training Accuracy:  59.38%\n",
            "Epoch: 4, Idx: 500, Training Loss: 0.6142, Training Accuracy:  71.88%\n",
            "Epoch: 04, Train Loss: 0.655, Train Acc: 60.45%, Val. Loss: 0.683174, Val. Acc: 53.34%\n",
            "Epoch: 5, Idx: 100, Training Loss: 0.6231, Training Accuracy:  65.62%\n",
            "Epoch: 5, Idx: 200, Training Loss: 0.7070, Training Accuracy:  65.62%\n",
            "Epoch: 5, Idx: 300, Training Loss: 0.5094, Training Accuracy:  78.12%\n",
            "Epoch: 5, Idx: 400, Training Loss: 0.3383, Training Accuracy:  84.38%\n",
            "Epoch: 5, Idx: 500, Training Loss: 0.5315, Training Accuracy:  71.88%\n",
            "Epoch: 05, Train Loss: 0.563, Train Acc: 70.60%, Val. Loss: 0.443561, Val. Acc: 79.18%\n",
            "Epoch: 6, Idx: 100, Training Loss: 0.5006, Training Accuracy:  75.00%\n",
            "Epoch: 6, Idx: 200, Training Loss: 0.5292, Training Accuracy:  65.62%\n",
            "Epoch: 6, Idx: 300, Training Loss: 0.4100, Training Accuracy:  81.25%\n",
            "Epoch: 6, Idx: 400, Training Loss: 0.4285, Training Accuracy:  78.12%\n",
            "Epoch: 6, Idx: 500, Training Loss: 0.5107, Training Accuracy:  71.88%\n",
            "Epoch: 06, Train Loss: 0.401, Train Acc: 82.03%, Val. Loss: 0.402303, Val. Acc: 81.20%\n",
            "Epoch: 7, Idx: 100, Training Loss: 0.2625, Training Accuracy:  90.62%\n",
            "Epoch: 7, Idx: 200, Training Loss: 0.3776, Training Accuracy:  84.38%\n",
            "Epoch: 7, Idx: 300, Training Loss: 0.4812, Training Accuracy:  81.25%\n",
            "Epoch: 7, Idx: 400, Training Loss: 0.3093, Training Accuracy:  84.38%\n",
            "Epoch: 7, Idx: 500, Training Loss: 0.4851, Training Accuracy:  78.12%\n",
            "Epoch: 07, Train Loss: 0.350, Train Acc: 84.64%, Val. Loss: 0.380656, Val. Acc: 82.19%\n",
            "Epoch: 8, Idx: 100, Training Loss: 0.2310, Training Accuracy:  93.75%\n",
            "Epoch: 8, Idx: 200, Training Loss: 0.1644, Training Accuracy:  93.75%\n",
            "Epoch: 8, Idx: 300, Training Loss: 0.2632, Training Accuracy:  87.50%\n",
            "Epoch: 8, Idx: 400, Training Loss: 0.3012, Training Accuracy:  87.50%\n",
            "Epoch: 8, Idx: 500, Training Loss: 0.2472, Training Accuracy:  90.62%\n",
            "Epoch: 08, Train Loss: 0.302, Train Acc: 87.32%, Val. Loss: 0.377120, Val. Acc: 82.55%\n",
            "Epoch: 9, Idx: 100, Training Loss: 0.4830, Training Accuracy:  81.25%\n",
            "Epoch: 9, Idx: 200, Training Loss: 0.1765, Training Accuracy:  93.75%\n",
            "Epoch: 9, Idx: 300, Training Loss: 0.2984, Training Accuracy:  84.38%\n",
            "Epoch: 9, Idx: 400, Training Loss: 0.2643, Training Accuracy:  90.62%\n",
            "Epoch: 9, Idx: 500, Training Loss: 0.2542, Training Accuracy:  84.38%\n",
            "Epoch: 09, Train Loss: 0.254, Train Acc: 89.48%, Val. Loss: 0.400060, Val. Acc: 82.76%\n",
            "Epoch: 10, Idx: 100, Training Loss: 0.1966, Training Accuracy:  90.62%\n",
            "Epoch: 10, Idx: 200, Training Loss: 0.2938, Training Accuracy:  87.50%\n",
            "Epoch: 10, Idx: 300, Training Loss: 0.2732, Training Accuracy:  90.62%\n",
            "Epoch: 10, Idx: 400, Training Loss: 0.1934, Training Accuracy:  93.75%\n",
            "Epoch: 10, Idx: 500, Training Loss: 0.1205, Training Accuracy:  93.75%\n",
            "Epoch: 10, Train Loss: 0.196, Train Acc: 92.32%, Val. Loss: 0.434544, Val. Acc: 81.20%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa351bZ6uSdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "154fb757-a8c1-4db7-dc9f-886b183a450f"
      },
      "source": [
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.438, Test Acc: 81.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJNSc5EowBuZ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb4QG1sKvpDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
        "\n",
        "test_sen1 = TEXT.preprocess(test_sen1)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "test_sen2 = TEXT.preprocess(test_sen2)\n",
        "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYaGYzu3vxa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d9df25f8-60fa-471f-85d1-c46c7ef77fdd"
      },
      "source": [
        "test_sen = np.asarray(test_sen2)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor, 1)\n",
        "out = F.softmax(output, 1)\n",
        "if (torch.argmax(out[0]) == 1):\n",
        "    print (\"Sentiment: Positive\")\n",
        "else:\n",
        "    print (\"Sentiment: Negative\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9irLisB6v4eW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "910b1b36-d241-4472-f0b4-e33cb45a3bec"
      },
      "source": [
        "test_sen = np.asarray(test_sen1)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor, 1)\n",
        "out = F.softmax(output, 1)\n",
        "if (torch.argmax(out[0]) == 1):\n",
        "    print (\"Sentiment: Positive\")\n",
        "else:\n",
        "    print (\"Sentiment: Negative\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Positive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP7cUhwBv9J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}