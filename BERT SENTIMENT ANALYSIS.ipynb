{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9294493345096666628\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7572807234333761833\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9517790666936853426\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7030817469046873939\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8295112569752283978\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:3\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1232649184395526399\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10988997837\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 14240356091433242069\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10988997837\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 1348825651888790864\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10988997837\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 9118845677775545164\n",
      "physical_device_desc: \"device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10462471783\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 9307917458643274620\n",
      "physical_device_desc: \"device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:2',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/home/purva/Desktop/purva/bert/bert/sample_text.txt\"\n",
    "bert_config_file=\"/home/purva/Desktop/purva/bert/uncased_L-12_H-768_A-12/bert_config.json\"\n",
    "\n",
    "task_name=\"MRPC\"\n",
    "\n",
    "output_dir=\"/home/purva/Desktop/purva/bert/output/\"\n",
    "init_checkpoint=\"/home/purva/Desktop/purva/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "do_lower_case= True\n",
    "max_seq_length= 128\n",
    "\n",
    "do_train=False\n",
    "do_eval=False\n",
    "do_predict= False\n",
    "train_batch_size= 32\n",
    "eval_batch_size=8\n",
    "predict_batch_size=8\n",
    "learning_rate= 5e-5\n",
    "num_train_epochs= 3.0\n",
    "warmup_proportion= 0.1\n",
    "save_checkpoints_steps= 1000\n",
    "\n",
    "iterations_per_loop=1000\n",
    "use_tpu= False\n",
    "tpu_name=None\n",
    "\n",
    "\n",
    "tpu_zone= None\n",
    "\n",
    "\n",
    "gcp_project= None\n",
    "master=None\n",
    "num_tpu_cores= 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    elif isinstance(text, unicode):\n",
    "      return text\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printable_text(text):\n",
    "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "  # These functions want `str` for both Python2 and Python3, but in one case\n",
    "  # it's a Unicode string and in the other it's a byte string.\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "  return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_by_vocab(vocab, items):\n",
    "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
    "  output = []\n",
    "  for item in items:\n",
    "    output.append(vocab[item])\n",
    "  return output\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "  return convert_by_vocab(vocab, tokens)\n",
    "\n",
    "\n",
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "  return convert_by_vocab(inv_vocab, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    self.vocab = vocab\n",
    "    self.unk_token = unk_token\n",
    "    self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "    using the given vocabulary.\n",
    "    For example:\n",
    "      input = \"unaffable\"\n",
    "      output = [\"un\", \"##aff\", \"##able\"]\n",
    "    Args:\n",
    "      text: A single token or whitespace separated tokens. This should have\n",
    "        already been passed through `BasicTokenizer.\n",
    "    Returns:\n",
    "      A list of wordpiece tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    text = convert_to_unicode(text)\n",
    "\n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "      chars = list(token)\n",
    "      if len(chars) > self.max_input_chars_per_word:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        continue\n",
    "\n",
    "      is_bad = False\n",
    "      start = 0\n",
    "      sub_tokens = []\n",
    "      while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "          substr = \"\".join(chars[start:end])\n",
    "          if start > 0:\n",
    "            substr = \"##\" + substr\n",
    "          if substr in self.vocab:\n",
    "            cur_substr = substr\n",
    "            break\n",
    "          end -= 1\n",
    "        if cur_substr is None:\n",
    "          is_bad = True\n",
    "          break\n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "\n",
    "      if is_bad:\n",
    "        output_tokens.append(self.unk_token)\n",
    "      else:\n",
    "        output_tokens.extend(sub_tokens)\n",
    "    return output_tokens\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "  # as whitespace since they are generally considered as such.\n",
    "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat == \"Zs\":\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "  # These are technically control characters but we count them as whitespace\n",
    "  # characters.\n",
    "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return False\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat in (\"Cc\", \"Cf\"):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "  cp = ord(char)\n",
    "  # We treat all non-letter/number ASCII as punctuation.\n",
    "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "  # Punctuation class but we treat them as punctuation anyways, for\n",
    "  # consistency.\n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat.startswith(\"P\"):\n",
    "    return True\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer(object):\n",
    "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_token)\n",
    "\n",
    "    return split_tokens\n",
    "\n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "  def __init__(self, do_lower_case=True):\n",
    "    \"\"\"Constructs a BasicTokenizer.\n",
    "    Args:\n",
    "      do_lower_case: Whether to lower case the input.\n",
    "    \"\"\"\n",
    "    self.do_lower_case = do_lower_case\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "\n",
    "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "    # models. This is also applied to the English models now, but it doesn't\n",
    "    # matter since the English models were not trained on any Chinese data\n",
    "    # and generally don't have any Chinese data in them (there are Chinese\n",
    "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "    # words in the English Wikipedia.).\n",
    "    text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "      if self.do_lower_case:\n",
    "        token = token.lower()\n",
    "        token = self._run_strip_accents(token)\n",
    "      split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "  def _run_strip_accents(self, text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cat = unicodedata.category(char)\n",
    "      if cat == \"Mn\":\n",
    "        continue\n",
    "      output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "  def _run_split_on_punc(self, text):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "      char = chars[i]\n",
    "      if _is_punctuation(char):\n",
    "        output.append([char])\n",
    "        start_new_word = True\n",
    "      else:\n",
    "        if start_new_word:\n",
    "          output.append([])\n",
    "        start_new_word = False\n",
    "        output[-1].append(char)\n",
    "      i += 1\n",
    "\n",
    "    return [\"\".join(x) for x in output]\n",
    "\n",
    "  def _tokenize_chinese_chars(self, text):\n",
    "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if self._is_chinese_char(cp):\n",
    "        output.append(\" \")\n",
    "        output.append(char)\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "  def _is_chinese_char(self, cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "      return True\n",
    "\n",
    "    return False\n",
    "\n",
    "  def _clean_text(self, text):\n",
    "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "        continue\n",
    "      if _is_whitespace(char):\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(object):\n",
    "  \"\"\"Configuration for `BertModel`.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               hidden_size=768,\n",
    "               num_hidden_layers=12,\n",
    "               num_attention_heads=12,\n",
    "               intermediate_size=3072,\n",
    "               hidden_act=\"gelu\",\n",
    "               hidden_dropout_prob=0.1,\n",
    "               attention_probs_dropout_prob=0.1,\n",
    "               max_position_embeddings=512,\n",
    "               type_vocab_size=16,\n",
    "               initializer_range=0.02):\n",
    "    \n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "\n",
    "  @classmethod\n",
    "  def from_dict(cls, json_object):\n",
    "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "    config = BertConfig(vocab_size=None)\n",
    "    for (key, value) in six.iteritems(json_object):\n",
    "      config.__dict__[key] = value\n",
    "    return config\n",
    "\n",
    "  @classmethod\n",
    "  def from_json_file(cls, json_file):\n",
    "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
    "      text = reader.read()\n",
    "    return cls.from_dict(json.loads(text))\n",
    "\n",
    "  def to_dict(self):\n",
    "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "    output = copy.deepcopy(self.__dict__)\n",
    "    return output\n",
    "\n",
    "  def to_json_string(self):\n",
    "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
    "  Example usage:\n",
    "  ```python\n",
    "  # Already been converted into WordPiece token ids\n",
    "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "  model = modeling.BertModel(config=config, is_training=True,\n",
    "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "  label_embeddings = tf.get_variable(...)\n",
    "  pooled_output = model.get_pooled_output()\n",
    "  logits = tf.matmul(pooled_output, label_embeddings)\n",
    "  ...\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               is_training,\n",
    "               input_ids,\n",
    "               input_mask=None,\n",
    "               token_type_ids=None,\n",
    "               use_one_hot_embeddings=False,\n",
    "               scope=None):\n",
    "    \"\"\"Constructor for BertModel.\n",
    "    Args:\n",
    "      config: `BertConfig` instance.\n",
    "      is_training: bool. true for training model, false for eval model. Controls\n",
    "        whether dropout will be applied.\n",
    "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
    "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
    "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "      scope: (optional) variable scope. Defaults to \"bert\".\n",
    "    Raises:\n",
    "      ValueError: The config is invalid or one of the input tensor shapes\n",
    "        is invalid.\n",
    "    \"\"\"\n",
    "    config = copy.deepcopy(config)\n",
    "    if not is_training:\n",
    "      config.hidden_dropout_prob = 0.0\n",
    "      config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "\n",
    "    if input_mask is None:\n",
    "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "    if token_type_ids is None:\n",
    "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "      with tf.variable_scope(\"embeddings\"):\n",
    "        # Perform embedding lookup on the word ids.\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "            input_ids=input_ids,\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.hidden_size,\n",
    "            initializer_range=config.initializer_range,\n",
    "            word_embedding_name=\"word_embeddings\",\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "        # Add positional embeddings and token type embeddings, then layer\n",
    "        # normalize and perform dropout.\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            input_tensor=self.embedding_output,\n",
    "            use_token_type=True,\n",
    "            token_type_ids=token_type_ids,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            token_type_embedding_name=\"token_type_embeddings\",\n",
    "            use_position_embeddings=True,\n",
    "            position_embedding_name=\"position_embeddings\",\n",
    "            initializer_range=config.initializer_range,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)\n",
    "\n",
    "      with tf.variable_scope(\"encoder\"):\n",
    "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
    "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
    "        # for the attention scores.\n",
    "        attention_mask = create_attention_mask_from_input_mask(\n",
    "            input_ids, input_mask)\n",
    "\n",
    "        # Run the stacked transformer.\n",
    "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "            input_tensor=self.embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_hidden_layers=config.num_hidden_layers,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            intermediate_act_fn=get_activation(config.hidden_act),\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "            initializer_range=config.initializer_range,\n",
    "            do_return_all_layers=True)\n",
    "\n",
    "      self.sequence_output = self.all_encoder_layers[-1]\n",
    "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
    "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
    "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
    "      # (or segment-pair-level) classification tasks where we need a fixed\n",
    "      # dimensional representation of the segment.\n",
    "      with tf.variable_scope(\"pooler\"):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token. We assume that this has been pre-trained\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor,\n",
    "            config.hidden_size,\n",
    "            activation=tf.tanh,\n",
    "            kernel_initializer=create_initializer(config.initializer_range))\n",
    "\n",
    "  def get_pooled_output(self):\n",
    "    return self.pooled_output\n",
    "\n",
    "  def get_sequence_output(self):\n",
    "    \"\"\"Gets final hidden layer of encoder.\n",
    "    Returns:\n",
    "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "      to the final hidden of the transformer encoder.\n",
    "    \"\"\"\n",
    "    return self.sequence_output\n",
    "\n",
    "  def get_all_encoder_layers(self):\n",
    "    return self.all_encoder_layers\n",
    "\n",
    "  def get_embedding_output(self):\n",
    "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
    "    Returns:\n",
    "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "      to the output of the embedding layer, after summing the word\n",
    "      embeddings with the positional embeddings and the token type embeddings,\n",
    "      then performing layer normalization. This is the input to the transformer.\n",
    "    \"\"\"\n",
    "    return self.embedding_output\n",
    "\n",
    "  def get_embedding_table(self):\n",
    "    return self.embedding_table\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "  \"\"\"Gaussian Error Linear Unit.\n",
    "  This is a smoother version of the RELU.\n",
    "  Original paper: https://arxiv.org/abs/1606.08415\n",
    "  Args:\n",
    "    x: float Tensor to perform activation.\n",
    "  Returns:\n",
    "    `x` with the GELU activation applied.\n",
    "  \"\"\"\n",
    "  cdf = 0.5 * (1.0 + tf.tanh(\n",
    "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "  return x * cdf\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
    "  Args:\n",
    "    activation_string: String name of the activation function.\n",
    "  Returns:\n",
    "    A Python function corresponding to the activation function. If\n",
    "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
    "    If `activation_string` is not a string, it will return `activation_string`.\n",
    "  Raises:\n",
    "    ValueError: The `activation_string` does not correspond to a known\n",
    "      activation.\n",
    "  \"\"\"\n",
    "\n",
    "  # We assume that anything that\"s not a string is already an activation\n",
    "  # function, so we just return it.\n",
    "  if not isinstance(activation_string, six.string_types):\n",
    "    return activation_string\n",
    "\n",
    "  if not activation_string:\n",
    "    return None\n",
    "\n",
    "  act = activation_string.lower()\n",
    "  if act == \"linear\":\n",
    "    return None\n",
    "  elif act == \"relu\":\n",
    "    return tf.nn.relu\n",
    "  elif act == \"gelu\":\n",
    "    return gelu\n",
    "  elif act == \"tanh\":\n",
    "    return tf.tanh\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
    "\n",
    "\n",
    "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
    "  assignment_map = {}\n",
    "  initialized_variable_names = {}\n",
    "\n",
    "  name_to_variable = collections.OrderedDict()\n",
    "  for var in tvars:\n",
    "    name = var.name\n",
    "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
    "    if m is not None:\n",
    "      name = m.group(1)\n",
    "    name_to_variable[name] = var\n",
    "\n",
    "  init_vars = tf.train.list_variables(init_checkpoint)\n",
    "\n",
    "  assignment_map = collections.OrderedDict()\n",
    "  for x in init_vars:\n",
    "    (name, var) = (x[0], x[1])\n",
    "    if name not in name_to_variable:\n",
    "      continue\n",
    "    assignment_map[name] = name\n",
    "    initialized_variable_names[name] = 1\n",
    "    initialized_variable_names[name + \":0\"] = 1\n",
    "\n",
    "  return (assignment_map, initialized_variable_names)\n",
    "\n",
    "\n",
    "def dropout(input_tensor, dropout_prob):\n",
    "  \"\"\"Perform dropout.\n",
    "  Args:\n",
    "    input_tensor: float Tensor.\n",
    "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
    "      *keeping* a dimension as in `tf.nn.dropout`).\n",
    "  Returns:\n",
    "    A version of `input_tensor` with dropout applied.\n",
    "  \"\"\"\n",
    "  if dropout_prob is None or dropout_prob == 0.0:\n",
    "    return input_tensor\n",
    "\n",
    "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
    "  return output\n",
    "\n",
    "\n",
    "def layer_norm(input_tensor, name=None):\n",
    "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "  return tf.contrib.layers.layer_norm(\n",
    "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
    "\n",
    "\n",
    "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
    "  \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
    "  output_tensor = layer_norm(input_tensor, name)\n",
    "  output_tensor = dropout(output_tensor, dropout_prob)\n",
    "  return output_tensor\n",
    "\n",
    "\n",
    "def create_initializer(initializer_range=0.02):\n",
    "  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
    "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
    "\n",
    "\n",
    "def embedding_lookup(input_ids,\n",
    "                     vocab_size,\n",
    "                     embedding_size=128,\n",
    "                     initializer_range=0.02,\n",
    "                     word_embedding_name=\"word_embeddings\",\n",
    "                     use_one_hot_embeddings=False):\n",
    "  \"\"\"Looks up words embeddings for id tensor.\n",
    "  Args:\n",
    "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
    "      ids.\n",
    "    vocab_size: int. Size of the embedding vocabulary.\n",
    "    embedding_size: int. Width of the word embeddings.\n",
    "    initializer_range: float. Embedding initialization range.\n",
    "    word_embedding_name: string. Name of the embedding table.\n",
    "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "      embeddings. If False, use `tf.gather()`.\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
    "  \"\"\"\n",
    "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
    "  # num_inputs].\n",
    "  #\n",
    "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "  # reshape to [batch_size, seq_length, 1].\n",
    "  if input_ids.shape.ndims == 2:\n",
    "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
    "\n",
    "  embedding_table = tf.get_variable(\n",
    "      name=word_embedding_name,\n",
    "      shape=[vocab_size, embedding_size],\n",
    "      initializer=create_initializer(initializer_range))\n",
    "\n",
    "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
    "  if use_one_hot_embeddings:\n",
    "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
    "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
    "  else:\n",
    "    output = tf.gather(embedding_table, flat_input_ids)\n",
    "\n",
    "  input_shape = get_shape_list(input_ids)\n",
    "\n",
    "  output = tf.reshape(output,\n",
    "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
    "  return (output, embedding_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_postprocessor(input_tensor,\n",
    "                            use_token_type=False,\n",
    "                            token_type_ids=None,\n",
    "                            token_type_vocab_size=16,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            use_position_embeddings=True,\n",
    "                            position_embedding_name=\"position_embeddings\",\n",
    "                            initializer_range=0.02,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1):\n",
    "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
    "  Args:\n",
    "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
    "      embedding_size].\n",
    "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
    "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      Must be specified if `use_token_type` is True.\n",
    "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
    "    token_type_embedding_name: string. The name of the embedding table variable\n",
    "      for token type ids.\n",
    "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
    "      position of each token in the sequence.\n",
    "    position_embedding_name: string. The name of the embedding table variable\n",
    "      for positional embeddings.\n",
    "    initializer_range: float. Range of the weight initialization.\n",
    "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
    "      used with this model. This can be longer than the sequence length of\n",
    "      input_tensor, but cannot be shorter.\n",
    "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
    "  Returns:\n",
    "    float tensor with same shape as `input_tensor`.\n",
    "  Raises:\n",
    "    ValueError: One of the tensor shapes or input values is invalid.\n",
    "  \"\"\"\n",
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  width = input_shape[2]\n",
    "\n",
    "  output = input_tensor\n",
    "\n",
    "  if use_token_type:\n",
    "    if token_type_ids is None:\n",
    "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
    "                       \"`use_token_type` is True.\")\n",
    "    token_type_table = tf.get_variable(\n",
    "        name=token_type_embedding_name,\n",
    "        shape=[token_type_vocab_size, width],\n",
    "        initializer=create_initializer(initializer_range))\n",
    "    # This vocab will be small so we always do one-hot here, since it is always\n",
    "    # faster for a small vocabulary.\n",
    "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
    "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
    "                                       [batch_size, seq_length, width])\n",
    "    output += token_type_embeddings\n",
    "\n",
    "  if use_position_embeddings:\n",
    "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "      full_position_embeddings = tf.get_variable(\n",
    "          name=position_embedding_name,\n",
    "          shape=[max_position_embeddings, width],\n",
    "          initializer=create_initializer(initializer_range))\n",
    "      # Since the position embedding table is a learned variable, we create it\n",
    "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
    "      # sequence length might be shorter than this, for faster training of\n",
    "      # tasks that do not have long sequences.\n",
    "      #\n",
    "      # So `full_position_embeddings` is effectively an embedding table\n",
    "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
    "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
    "      # perform a slice.\n",
    "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
    "                                     [seq_length, -1])\n",
    "      num_dims = len(output.shape.as_list())\n",
    "\n",
    "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
    "      # we broadcast among the first dimensions, which is typically just\n",
    "      # the batch size.\n",
    "      position_broadcast_shape = []\n",
    "      for _ in range(num_dims - 2):\n",
    "        position_broadcast_shape.append(1)\n",
    "      position_broadcast_shape.extend([seq_length, width])\n",
    "      position_embeddings = tf.reshape(position_embeddings,\n",
    "                                       position_broadcast_shape)\n",
    "      output += position_embeddings\n",
    "\n",
    "  output = layer_norm_and_dropout(output, dropout_prob)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
    "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
    "  Args:\n",
    "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
    "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
    "  \"\"\"\n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  batch_size = from_shape[0]\n",
    "  from_seq_length = from_shape[1]\n",
    "\n",
    "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "  to_seq_length = to_shape[1]\n",
    "\n",
    "  to_mask = tf.cast(\n",
    "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
    "\n",
    "  # We don't assume that `from_tensor` is a mask (although it could be). We\n",
    "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
    "  # tokens so we create a tensor of all ones.\n",
    "  #\n",
    "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
    "  broadcast_ones = tf.ones(\n",
    "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
    "\n",
    "  # Here we broadcast along two dimensions to create the mask.\n",
    "  mask = broadcast_ones * to_mask\n",
    "\n",
    "  return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
    "  This is an implementation of multi-headed attention based on \"Attention\n",
    "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
    "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
    "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
    "  This function first projects `from_tensor` into a \"query\" tensor and\n",
    "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
    "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
    "  [batch_size, seq_length, size_per_head].\n",
    "  Then, the query and key tensors are dot-producted and scaled. These are\n",
    "  softmaxed to obtain attention probabilities. The value tensors are then\n",
    "  interpolated by these probabilities, then concatenated back to a single\n",
    "  tensor and returned.\n",
    "  In practice, the multi-headed attention are done with transposes and\n",
    "  reshapes rather than actual separate tensors.\n",
    "  Args:\n",
    "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
    "      from_width].\n",
    "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
    "    attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
    "      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
    "      attention scores will effectively be set to -infinity for any positions in\n",
    "      the mask that are 0, and will be unchanged for positions that are 1.\n",
    "    num_attention_heads: int. Number of attention heads.\n",
    "    size_per_head: int. Size of each attention head.\n",
    "    query_act: (optional) Activation function for the query transform.\n",
    "    key_act: (optional) Activation function for the key transform.\n",
    "    value_act: (optional) Activation function for the value transform.\n",
    "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
    "      attention probabilities.\n",
    "    initializer_range: float. Range of the weight initializer.\n",
    "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
    "      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
    "      output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
    "      * size_per_head].\n",
    "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
    "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
    "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `from_tensor`.\n",
    "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `to_tensor`.\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, from_seq_length,\n",
    "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
    "      true, this will be of shape [batch_size * from_seq_length,\n",
    "      num_attention_heads * size_per_head]).\n",
    "  Raises:\n",
    "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
    "  \"\"\"\n",
    "\n",
    "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
    "                           seq_length, width):\n",
    "    output_tensor = tf.reshape(\n",
    "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "\n",
    "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "    return output_tensor\n",
    "\n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
    "\n",
    "  if len(from_shape) != len(to_shape):\n",
    "    raise ValueError(\n",
    "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
    "\n",
    "  if len(from_shape) == 3:\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    to_seq_length = to_shape[1]\n",
    "  elif len(from_shape) == 2:\n",
    "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "      raise ValueError(\n",
    "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
    "          \"must all be specified.\")\n",
    "\n",
    "  # Scalar dimensions referenced here:\n",
    "  #   B = batch size (number of sequences)\n",
    "  #   F = `from_tensor` sequence length\n",
    "  #   T = `to_tensor` sequence length\n",
    "  #   N = `num_attention_heads`\n",
    "  #   H = `size_per_head`\n",
    "\n",
    "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
    "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
    "\n",
    "  # `query_layer` = [B*F, N*H]\n",
    "  query_layer = tf.layers.dense(\n",
    "      from_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=query_act,\n",
    "      name=\"query\",\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "  # `key_layer` = [B*T, N*H]\n",
    "  key_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=key_act,\n",
    "      name=\"key\",\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "  # `value_layer` = [B*T, N*H]\n",
    "  value_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=value_act,\n",
    "      name=\"value\",\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "  # `query_layer` = [B, N, F, H]\n",
    "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
    "                                     num_attention_heads, from_seq_length,\n",
    "                                     size_per_head)\n",
    "\n",
    "  # `key_layer` = [B, N, T, H]\n",
    "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
    "                                   to_seq_length, size_per_head)\n",
    "\n",
    "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "  # attention scores.\n",
    "  # `attention_scores` = [B, N, F, T]\n",
    "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "  attention_scores = tf.multiply(attention_scores,\n",
    "                                 1.0 / math.sqrt(float(size_per_head)))\n",
    "\n",
    "  if attention_mask is not None:\n",
    "    # `attention_mask` = [B, 1, F, T]\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "\n",
    "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "    # masked positions, this operation will create a tensor which is 0.0 for\n",
    "    # positions we want to attend and -10000.0 for masked positions.\n",
    "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "\n",
    "    # Since we are adding it to the raw scores before the softmax, this is\n",
    "    # effectively the same as removing these entirely.\n",
    "    attention_scores += adder\n",
    "\n",
    "  # Normalize the attention scores to probabilities.\n",
    "  # `attention_probs` = [B, N, F, T]\n",
    "  attention_probs = tf.nn.softmax(attention_scores)\n",
    "\n",
    "  # This is actually dropping out entire tokens to attend to, which might\n",
    "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
    "\n",
    "  # `value_layer` = [B, T, N, H]\n",
    "  value_layer = tf.reshape(\n",
    "      value_layer,\n",
    "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
    "\n",
    "  # `value_layer` = [B, N, T, H]\n",
    "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
    "\n",
    "  # `context_layer` = [B, N, F, H]\n",
    "  context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "  # `context_layer` = [B, F, N, H]\n",
    "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "\n",
    "  if do_return_2d_tensor:\n",
    "    # `context_layer` = [B*F, N*H]\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
    "  else:\n",
    "    # `context_layer` = [B, F, N*H]\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
    "\n",
    "  return context_layer\n",
    "\n",
    "\n",
    "def transformer_model(input_tensor,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "  \n",
    "  if hidden_size % num_attention_heads != 0:\n",
    "    raise ValueError(\n",
    "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "\n",
    "  attention_head_size = int(hidden_size / num_attention_heads)\n",
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  input_width = input_shape[2]\n",
    "\n",
    "  # The Transformer performs sum residuals on all layers so the input needs\n",
    "  # to be the same as the hidden size.\n",
    "  if input_width != hidden_size:\n",
    "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
    "                     (input_width, hidden_size))\n",
    "\n",
    "  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
    "  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
    "  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
    "  # help the optimizer.\n",
    "  prev_output = reshape_to_matrix(input_tensor)\n",
    "\n",
    "  all_layer_outputs = []\n",
    "  for layer_idx in range(num_hidden_layers):\n",
    "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
    "      layer_input = prev_output\n",
    "\n",
    "      with tf.variable_scope(\"attention\"):\n",
    "        attention_heads = []\n",
    "        with tf.variable_scope(\"self\"):\n",
    "          attention_head = attention_layer(\n",
    "              from_tensor=layer_input,\n",
    "              to_tensor=layer_input,\n",
    "              attention_mask=attention_mask,\n",
    "              num_attention_heads=num_attention_heads,\n",
    "              size_per_head=attention_head_size,\n",
    "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "              initializer_range=initializer_range,\n",
    "              do_return_2d_tensor=True,\n",
    "              batch_size=batch_size,\n",
    "              from_seq_length=seq_length,\n",
    "              to_seq_length=seq_length)\n",
    "          attention_heads.append(attention_head)\n",
    "\n",
    "        attention_output = None\n",
    "        if len(attention_heads) == 1:\n",
    "          attention_output = attention_heads[0]\n",
    "        else:\n",
    "          # In the case where we have other sequences, we just concatenate\n",
    "          # them to the self-attention head before the projection.\n",
    "          attention_output = tf.concat(attention_heads, axis=-1)\n",
    "\n",
    "        # Run a linear projection of `hidden_size` then add a residual\n",
    "        # with `layer_input`.\n",
    "        with tf.variable_scope(\"output\"):\n",
    "          attention_output = tf.layers.dense(\n",
    "              attention_output,\n",
    "              hidden_size,\n",
    "              kernel_initializer=create_initializer(initializer_range))\n",
    "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
    "          attention_output = layer_norm(attention_output + layer_input)\n",
    "\n",
    "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
    "      with tf.variable_scope(\"intermediate\"):\n",
    "        intermediate_output = tf.layers.dense(\n",
    "            attention_output,\n",
    "            intermediate_size,\n",
    "            activation=intermediate_act_fn,\n",
    "            kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "      # Down-project back to `hidden_size` then add the residual.\n",
    "      with tf.variable_scope(\"output\"):\n",
    "        layer_output = tf.layers.dense(\n",
    "            intermediate_output,\n",
    "            hidden_size,\n",
    "            kernel_initializer=create_initializer(initializer_range))\n",
    "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
    "        layer_output = layer_norm(layer_output + attention_output)\n",
    "        prev_output = layer_output\n",
    "        all_layer_outputs.append(layer_output)\n",
    "\n",
    "  if do_return_all_layers:\n",
    "    final_outputs = []\n",
    "    for layer_output in all_layer_outputs:\n",
    "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "      final_outputs.append(final_output)\n",
    "    return final_outputs\n",
    "  else:\n",
    "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "  Args:\n",
    "    tensor: A tf.Tensor object to find the shape of.\n",
    "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "      specified and the `tensor` has a different rank, and exception will be\n",
    "      thrown.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "  Returns:\n",
    "    A list of dimensions of the shape of tensor. All static dimensions will\n",
    "    be returned as python integers, and dynamic dimensions will be returned\n",
    "    as tf.Tensor scalars.\n",
    "  \"\"\"\n",
    "  if name is None:\n",
    "    name = tensor.name\n",
    "\n",
    "  if expected_rank is not None:\n",
    "    assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "  shape = tensor.shape.as_list()\n",
    "\n",
    "  non_static_indexes = []\n",
    "  for (index, dim) in enumerate(shape):\n",
    "    if dim is None:\n",
    "      non_static_indexes.append(index)\n",
    "\n",
    "  if not non_static_indexes:\n",
    "    return shape\n",
    "\n",
    "  dyn_shape = tf.shape(tensor)\n",
    "  for index in non_static_indexes:\n",
    "    shape[index] = dyn_shape[index]\n",
    "  return shape\n",
    "\n",
    "\n",
    "def reshape_to_matrix(input_tensor):\n",
    "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
    "  ndims = input_tensor.shape.ndims\n",
    "  if ndims < 2:\n",
    "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                     (input_tensor.shape))\n",
    "  if ndims == 2:\n",
    "    return input_tensor\n",
    "\n",
    "  width = input_tensor.shape[-1]\n",
    "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "  return output_tensor\n",
    "\n",
    "\n",
    "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
    "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
    "  if len(orig_shape_list) == 2:\n",
    "    return output_tensor\n",
    "\n",
    "  output_shape = get_shape_list(output_tensor)\n",
    "\n",
    "  orig_dims = orig_shape_list[0:-1]\n",
    "  width = output_shape[-1]\n",
    "\n",
    "  return tf.reshape(output_tensor, orig_dims + [width])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_rank(tensor, expected_rank, name=None):\n",
    "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
    "  Args:\n",
    "    tensor: A tf.Tensor to check the rank of.\n",
    "    expected_rank: Python integer or list of integers, expected rank.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "  Raises:\n",
    "    ValueError: If the expected shape doesn't match the actual shape.\n",
    "  \"\"\"\n",
    "  if name is None:\n",
    "    name = tensor.name\n",
    "\n",
    "  expected_rank_dict = {}\n",
    "  if isinstance(expected_rank, six.integer_types):\n",
    "    expected_rank_dict[expected_rank] = True\n",
    "  else:\n",
    "    for x in expected_rank:\n",
    "      expected_rank_dict[x] = True\n",
    "\n",
    "  actual_rank = tensor.shape.ndims\n",
    "  if actual_rank not in expected_rank_dict:\n",
    "    scope_name = tf.get_variable_scope().name\n",
    "    raise ValueError(\n",
    "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
    "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
    "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
    "  \"\"\"Creates an optimizer training op.\"\"\"\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "\n",
    "  # Implements linear decay of the learning rate.\n",
    "  learning_rate = tf.train.polynomial_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      num_train_steps,\n",
    "      end_learning_rate=0.0,\n",
    "      power=1.0,\n",
    "      cycle=False)\n",
    "\n",
    "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
    "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "  if num_warmup_steps:\n",
    "    global_steps_int = tf.cast(global_step, tf.int32)\n",
    "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "    warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "    learning_rate = (\n",
    "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "  # It is recommended that you use this optimizer for fine tuning, since this\n",
    "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
    "  # loaded from init_checkpoint.)\n",
    "  optimizer = AdamWeightDecayOptimizer(\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay_rate=0.01,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-6,\n",
    "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "  if use_tpu:\n",
    "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "  tvars = tf.trainable_variables()\n",
    "  grads = tf.gradients(loss, tvars)\n",
    "\n",
    "  # This is how the model was pre-trained.\n",
    "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "  train_op = optimizer.apply_gradients(\n",
    "      zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "  # Normally the global step update is done inside of `apply_gradients`.\n",
    "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
    "  # a different optimizer, you should probably take this line out.\n",
    "  new_global_step = global_step + 1\n",
    "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "  return train_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               weight_decay_rate=0.0,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=1e-6,\n",
    "               exclude_from_weight_decay=None,\n",
    "               name=\"AdamWeightDecayOptimizer\"):\n",
    "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
    "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.weight_decay_rate = weight_decay_rate\n",
    "    self.beta_1 = beta_1\n",
    "    self.beta_2 = beta_2\n",
    "    self.epsilon = epsilon\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    assignments = []\n",
    "    for (grad, param) in grads_and_vars:\n",
    "      if grad is None or param is None:\n",
    "        continue\n",
    "\n",
    "      param_name = self._get_variable_name(param.name)\n",
    "\n",
    "      m = tf.get_variable(\n",
    "          name=param_name + \"/adam_m\",\n",
    "          shape=param.shape.as_list(),\n",
    "          dtype=tf.float32,\n",
    "          trainable=False,\n",
    "          initializer=tf.zeros_initializer())\n",
    "      v = tf.get_variable(\n",
    "          name=param_name + \"/adam_v\",\n",
    "          shape=param.shape.as_list(),\n",
    "          dtype=tf.float32,\n",
    "          trainable=False,\n",
    "          initializer=tf.zeros_initializer())\n",
    "\n",
    "      # Standard Adam update.\n",
    "      next_m = (\n",
    "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
    "      next_v = (\n",
    "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
    "                                                    tf.square(grad)))\n",
    "\n",
    "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
    "\n",
    "      # Just adding the square of the weights to the loss function is *not*\n",
    "      # the correct way of using L2 regularization/weight decay with Adam,\n",
    "      # since that will interact with the m and v parameters in strange ways.\n",
    "      #\n",
    "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
    "      # with the m/v parameters. This is equivalent to adding the square\n",
    "      # of the weights to the loss with plain (non-momentum) SGD.\n",
    "      if self._do_use_weight_decay(param_name):\n",
    "        update += self.weight_decay_rate * param\n",
    "\n",
    "      update_with_lr = self.learning_rate * update\n",
    "\n",
    "      next_param = param - update_with_lr\n",
    "\n",
    "      assignments.extend(\n",
    "          [param.assign(next_param),\n",
    "           m.assign(next_m),\n",
    "           v.assign(next_v)])\n",
    "    return tf.group(*assignments, name=name)\n",
    "\n",
    "  def _do_use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay_rate:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _get_variable_name(self, param_name):\n",
    "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
    "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
    "    if m is not None:\n",
    "      param_name = m.group(1)\n",
    "    return param_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "    \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    self.is_real_example = is_real_example\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      return lines\n",
    "\n",
    "\n",
    "class XnliProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the XNLI data set.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.language = \"zh\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    lines = self._read_tsv(\n",
    "        os.path.join(data_dir, \"multinli\",\n",
    "                     \"multinli.train.%s.tsv\" % self.language))\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"train-%d\" % (i)\n",
    "      text_a = tokenization.convert_to_unicode(line[0])\n",
    "      text_b = tokenization.convert_to_unicode(line[1])\n",
    "      label = tokenization.convert_to_unicode(line[2])\n",
    "      if label == tokenization.convert_to_unicode(\"contradictory\"):\n",
    "        label = tokenization.convert_to_unicode(\"contradiction\")\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    lines = self._read_tsv(os.path.join(data_dir, \"xnli.dev.tsv\"))\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"dev-%d\" % (i)\n",
    "      language = tokenization.convert_to_unicode(line[0])\n",
    "      if language != tokenization.convert_to_unicode(self.language):\n",
    "        continue\n",
    "      text_a = tokenization.convert_to_unicode(line[6])\n",
    "      text_b = tokenization.convert_to_unicode(line[7])\n",
    "      label = tokenization.convert_to_unicode(line[1])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "\n",
    "class MnliProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
    "        \"dev_matched\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[0]))\n",
    "      text_a = tokenization.convert_to_unicode(line[8])\n",
    "      text_b = tokenization.convert_to_unicode(line[9])\n",
    "      if set_type == \"test\":\n",
    "        label = \"contradiction\"\n",
    "      else:\n",
    "        label = tokenization.convert_to_unicode(line[-1])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "\n",
    "class MrpcProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      text_a = tokenization.convert_to_unicode(line[3])\n",
    "      text_b = tokenization.convert_to_unicode(line[4])\n",
    "      if set_type == \"test\":\n",
    "        label = \"0\"\n",
    "      else:\n",
    "        label = tokenization.convert_to_unicode(line[0])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "\n",
    "class ColaProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      # Only the test set has a header\n",
    "      if set_type == \"test\" and i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      if set_type == \"test\":\n",
    "        text_a = tokenization.convert_to_unicode(line[1])\n",
    "        label = \"0\"\n",
    "      else:\n",
    "        text_a = tokenization.convert_to_unicode(line[3])\n",
    "        label = tokenization.convert_to_unicode(line[1])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "  if isinstance(example, PaddingInputExample):\n",
    "    return InputFeatures(\n",
    "        input_ids=[0] * max_seq_length,\n",
    "        input_mask=[0] * max_seq_length,\n",
    "        segment_ids=[0] * max_seq_length,\n",
    "        label_id=0,\n",
    "        is_real_example=False)\n",
    "\n",
    "  label_map = {}\n",
    "  for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(example.text_a)\n",
    "  tokens_b = None\n",
    "  if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "  if tokens_b:\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "  else:\n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "  # The convention in BERT is:\n",
    "  # (a) For sequence pairs:\n",
    "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "  # (b) For single sequences:\n",
    "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "  #  type_ids: 0     0   0   0  0     0 0\n",
    "  #\n",
    "  # Where \"type_ids\" are used to indicate whether this is the first\n",
    "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "  # embedding vector (and position vector). This is not *strictly* necessary\n",
    "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "  # it easier for the model to learn the concept of sequences.\n",
    "  #\n",
    "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "  # used as the \"sentence vector\". Note that this only makes sense because\n",
    "  # the entire model is fine-tuned.\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "\n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 5:\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [printable_text(x) for x in tokens]))\n",
    "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "  feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)\n",
    "  return feature\n",
    "\n",
    "\n",
    "def file_based_convert_examples_to_features(\n",
    "    examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "  writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    if ex_index % 10000 == 0:\n",
    "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "    feature = convert_single_example(ex_index, example, label_list,\n",
    "                                     max_seq_length, tokenizer)\n",
    "\n",
    "    def create_int_feature(values):\n",
    "      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "      return f\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "    features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "    features[\"is_real_example\"] = create_int_feature(\n",
    "        [int(feature.is_real_example)])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "  writer.close()\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "  # This is a simple heuristic which will always truncate the longer sequence\n",
    "  # one token at a time. This makes more sense than truncating an equal percent\n",
    "  # of tokens from each, since if one sequence is very short then each token\n",
    "  # that's truncated likely contains more information than a longer sequence.\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  # In the demo, we are doing a simple classification task on the entire\n",
    "  # segment.\n",
    "  #\n",
    "  # If you want to use the token-level output, use model.get_sequence_output()\n",
    "  # instead.\n",
    "  output_layer = model.get_pooled_output()\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "    if is_training:\n",
    "      # I.e., 0.1 dropout\n",
    "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "    is_real_example = None\n",
    "    if \"is_real_example\" in features:\n",
    "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "    else:\n",
    "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "        num_labels, use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "      train_op = create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(\n",
    "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"eval_loss\": loss,\n",
    "        }\n",
    "\n",
    "      eval_metrics = (metric_fn,\n",
    "                      [per_example_loss, label_ids, logits, is_real_example])\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions={\"probabilities\": probabilities},\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "# This function is not used by this file but is still used by the Colab and\n",
    "# people who depend on it.\n",
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer):\n",
    "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    if ex_index % 10000 == 0:\n",
    "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "    feature = convert_single_example(ex_index, example, label_list,\n",
    "                                     max_seq_length, tokenizer)\n",
    "\n",
    "    features.append(feature)\n",
    "  return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/home/purva/Desktop/purva/bert/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=None\n",
    "output_file=\"/home/purva/Desktop/purva/bert/output/out.bin\"\n",
    "vocab_file=\"/home/purva/Desktop/purva/bert/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "do_lower_case=True\n",
    "do_whole_word_mask= False\n",
    "max_seq_length= 128\n",
    "max_predictions_per_seq= 20\n",
    "random_seed= 12345\n",
    "dupe_factor=10\n",
    "short_seq_prob= 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: /home/purva/Desktop/purva/bert/output/ *****\n"
     ]
    }
   ],
   "source": [
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "  def __str__(self):\n",
    "    s = \"\"\n",
    "    s += \"tokens: %s\\n\" % (\" \".join(\n",
    "        [printable_text(x) for x in self.tokens]))\n",
    "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.masked_lm_positions]))\n",
    "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "        [printable_text(x) for x in self.masked_lm_labels]))\n",
    "    s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()\n",
    "\n",
    "\n",
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        tf.logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  tf.logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n",
    "\n",
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "  all_documents = [[]]\n",
    "\n",
    "  # Input file format:\n",
    "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "  # sentence boundaries for the \"next sentence prediction\" task).\n",
    "  # (2) Blank lines between documents. Document boundaries are needed so\n",
    "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = tokenization.convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(tokenizer.vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances\n",
    "\n",
    "\n",
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "\n",
    "  # We *usually* want to fill up the entire sequence since we are padding\n",
    "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "  # computation. However, we *sometimes*\n",
    "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "  # The `target_seq_length` is just a rough target however, whereas\n",
    "  # `max_seq_length` is a hard limit.\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  # We DON'T just concatenate all of the tokens from a document into a long\n",
    "  # sequence and choose an arbitrary split point because this would make the\n",
    "  # next sentence prediction task too easy. Instead, we split the input into\n",
    "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "  # input.\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        # Actual next\n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "\n",
    "  return instances\n",
    "\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      continue\n",
    "   \n",
    "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "\n",
    "  rng.shuffle(cand_indexes)\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "  input_files = []\n",
    "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "  tf.logging.info(\"*** Reading from input files ***\")\n",
    "  for input_file in input_files:\n",
    "    tf.logging.info(\"  %s\", input_file)\n",
    "\n",
    "  rng = random.Random(FLAGS.random_seed)\n",
    "  instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "\n",
    "  output_files = FLAGS.output_file.split(\",\")\n",
    "  tf.logging.info(\"*** Writing to output files ***\")\n",
    "  for output_file in output_files:\n",
    "    tf.logging.info(\"  %s\", output_file)\n",
    "\n",
    "  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn =input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 12:57:25.443612 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 12:57:36.459872 139639324784384 deprecation.py:506] From <ipython-input-47-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0723 12:57:36.500020 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0723 12:57:36.666548 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0723 12:57:42.175130 139639324784384 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0723 12:57:45.328771 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0723 12:57:47.423034 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "W0723 13:16:03.229643 139639324784384 deprecation.py:323] From /home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:18:39.066722\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/purva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.88684005,\n",
       " 'eval_accuracy': 0.88684,\n",
       " 'f1_score': 0.8860514,\n",
       " 'false_negatives': 1501.0,\n",
       " 'false_positives': 1328.0,\n",
       " 'loss': 0.4772855,\n",
       " 'precision': 0.892269,\n",
       " 'recall': 0.87992,\n",
       " 'true_negatives': 11172.0,\n",
       " 'true_positives': 10999.0,\n",
       " 'global_step': 2343}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  labels = [\"Negative\", \"Positive\"]\n",
    "  input_examples = [InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That movie was absolutely awful',\n",
       "  array([-0.00639026, -5.0561767 ], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The acting was a bit lacking',\n",
       "  array([-0.08855274, -2.468106  ], dtype=float32),\n",
       "  'Negative'),\n",
       " ('The film was creative and surprising',\n",
       "  array([-6.4774642e+00, -1.5388802e-03], dtype=float32),\n",
       "  'Positive'),\n",
       " ('Absolutely fantastic!',\n",
       "  array([-5.7236996e+00, -3.2728936e-03], dtype=float32),\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
