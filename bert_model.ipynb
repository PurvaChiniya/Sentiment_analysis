{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/purva/Desktop/purva/bert'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    " \n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"/home/purva/Desktop/purva/bert/bert.txt\"\n",
    "output_file=\"/tmp/tf_examples.tfrecord\" \n",
    "vocab_file=\"/home/purva/Desktop/purva/bert/uncased_L-12_H-768_A-12/vocab.txt\" \n",
    "do_lower_case=True\n",
    "\n",
    "do_whole_word_mask= False\n",
    "max_seq_length= 128\n",
    "max_predictions_per_seq= 20\n",
    "random_seed= 12345\n",
    "dupe_factor =10\n",
    "masked_lm_prob= 0.15\n",
    "short_seq_prob= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0709 04:24:00.528540 140445336774400 <ipython-input-5-10de1051c4c8>:5] *** Reading from input files ***\n",
      "I0709 04:24:00.529668 140445336774400 <ipython-input-5-10de1051c4c8>:7]   /home/purva/Desktop/purva/bert/bert.txt\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "for input_pattern in input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "tf.logging.info(\"*** Reading from input files ***\")\n",
    "for input_file in input_files:\n",
    "    tf.logging.info(\"  %s\", input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<random.Random at 0x55d4578bc198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.Random(random_seed)\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/purva/Desktop/purva/bert/bert.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gfile.Glob(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to unicode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    elif isinstance(text, unicode):\n",
    "      return text\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printable_text(text):\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=load_vocab(vocab_file)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_by_vocab(vocab, items):\n",
    "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
    "  output = []\n",
    "  for item in items:\n",
    "    output.append(vocab[item])\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "  return convert_by_vocab(vocab, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "  return convert_by_vocab(inv_vocab, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(object):\n",
    "  def __init__(self, do_lower_case=True):\n",
    "    self.do_lower_case = do_lower_case\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "    \n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "      if self.do_lower_case:\n",
    "        token = token.lower()\n",
    "        token = self._run_strip_accents(token)\n",
    "      split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "  def _run_strip_accents(self, text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cat = unicodedata.category(char)\n",
    "      if cat == \"Mn\":\n",
    "        continue\n",
    "      output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "  def _run_split_on_punc(self, text):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "      char = chars[i]\n",
    "      if _is_punctuation(char):\n",
    "        output.append([char])\n",
    "        start_new_word = True\n",
    "      else:\n",
    "        if start_new_word:\n",
    "          output.append([])\n",
    "        start_new_word = False\n",
    "        output[-1].append(char)\n",
    "      i += 1\n",
    "\n",
    "    return [\"\".join(x) for x in output]\n",
    "  def _clean_text(self, text):\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "        continue\n",
    "      if _is_whitespace(char):\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'was', 'jim', 'henson', '?', 'jim', 'henson', 'was', 'a', 'puppeteer']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "basic_tokenizer.tokenize(\"Who was Jim Henson ? Jim Henson was a puppeteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDPIECE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    self.vocab = vocab\n",
    "    self.unk_token = unk_token\n",
    "    self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    text = convert_to_unicode(text)\n",
    "\n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "      chars = list(token)\n",
    "      if len(chars) > self.max_input_chars_per_word:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        continue\n",
    "\n",
    "      is_bad = False\n",
    "      start = 0\n",
    "      sub_tokens = []\n",
    "      while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "          substr = \"\".join(chars[start:end])\n",
    "          if start > 0:\n",
    "            substr = \"##\" + substr\n",
    "          if substr in self.vocab:\n",
    "            cur_substr = substr\n",
    "            break\n",
    "          end -= 1\n",
    "        if cur_substr is None:\n",
    "          is_bad = True\n",
    "          break\n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "\n",
    "      if is_bad:\n",
    "        output_tokens.append(self.unk_token)\n",
    "      else:\n",
    "        output_tokens.extend(sub_tokens)\n",
    "    return output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'was',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '?',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer=WordpieceTokenizer(vocab=vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200)\n",
    "wordpiece_tokenizer.tokenize(\"Who was Jim Henson ? Jim Henson was a puppeteer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer(object):\n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_token)\n",
    "\n",
    "    return split_tokens\n",
    "\n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(char):\n",
    "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat == \"Zs\":\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_control(char):\n",
    "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return False\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat in (\"Cc\", \"Cf\"):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_punctuation(char):\n",
    "  cp = ord(char)\n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat.startswith(\"P\"):\n",
    "    return True\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=FullTokenizer( vocab_file=vocab_file, do_lower_case=True)\n",
    "tokenizer.tokenize(\"Who was Jim Henson ? Jim Henson was a puppeteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "tokenizer = FullTokenizer(\n",
    "  vocab_file=vocab_file, do_lower_case=do_lower_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:24.809024 140445336774400 <ipython-input-53-735999f785b2>:5] *** Reading from input files ***\n",
      "I0709 04:27:24.809899 140445336774400 <ipython-input-53-735999f785b2>:7]   /home/purva/Desktop/purva/bert/bert.txt\n"
     ]
    }
   ],
   "source": [
    "input_files = []\n",
    "for input_pattern in input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "    \n",
    "tf.logging.info(\"*** Reading from input files ***\")\n",
    "for input_file in input_files:\n",
    "    tf.logging.info(\"  %s\", input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \n",
    "  all_documents = [[]]\n",
    "\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/purva/Desktop/purva/bert/bert.txt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = [[]]\n",
    "\n",
    "for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "# Remove empty documents\n",
    "all_documents = [x for x in all_documents if x]\n",
    "rng.shuffle(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'fountain',\n",
       " 'of',\n",
       " 'classic',\n",
       " 'wisdom',\n",
       " ',',\n",
       " 'h',\n",
       " '##yp',\n",
       " '##ati',\n",
       " '##a',\n",
       " 'herself',\n",
       " '.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = list(vocab.keys())\n",
    "#vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word. When a word has been split into\n",
    "    # WordPieces, the first token does not have any marker and any subsequence\n",
    "    # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "    # append it to the previous set of word indexes.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "\n",
    "  rng.shuffle(cand_indexes)\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  # We DON'T just concatenate all of the tokens from a document into a long\n",
    "  # sequence and choose an arbitrary split point because this would make the\n",
    "  # next sentence prediction task too easy. Instead, we split the input into\n",
    "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "  # input.\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "         \n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "          \n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        \n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "\n",
    "  return instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "  def __str__(self):\n",
    "    s = \"\"\n",
    "    s += \"tokens: %s\\n\" % (\" \".join(\n",
    "        [printable_text(x) for x in self.tokens]))\n",
    "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.masked_lm_positions]))\n",
    "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "        [printable_text(x) for x in self.masked_lm_labels]))\n",
    "    s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "rng.shuffle(instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens: [CLS] \" ##lass \" beard had [MASK] early that [MASK] , but [MASK] with a view to [MASK] . a leak in his cabin roof , - - quite consistent with his careless [MASK] imp ##rov [MASK] ##nt habits [MASK] arcadia - had rouse ##d [MASK] at 4 a . m . , with a flooded \" bunk \" and wet blankets . [SEP] text should be one - sentence - [MASK] - line , with empty lines between documents . [MASK] sample text [MASK] [MASK] domain and was randomly selected from project gut ##tenberg . [SEP]\n",
       "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "is_random_next: True\n",
       "masked_lm_positions: 2 6 9 12 17 22 33 36 39 40 45 71 81 84 85\n",
       "masked_lm_labels: cass risen morning not discovery his , ##ide , - him per this is public\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:27.820129 140445336774400 <ipython-input-67-382b8c9e2ba7>:2] *** Writing to output files ***\n",
      "I0709 04:27:27.820868 140445336774400 <ipython-input-67-382b8c9e2ba7>:4]   /tmp/tf_examples.tfrecord\n"
     ]
    }
   ],
   "source": [
    "output_files = output_file.split(\",\")\n",
    "tf.logging.info(\"*** Writing to output files ***\")\n",
    "for output_file in output_files:\n",
    "    tf.logging.info(\"  %s\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        tf.logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  tf.logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.535924 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.536707 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] \" ##lass \" beard had [MASK] early that [MASK] , but [MASK] with a view to [MASK] . a leak in his cabin roof , - - quite consistent with his careless [MASK] imp ##rov [MASK] ##nt habits [MASK] arcadia - had rouse ##d [MASK] at 4 a . m . , with a flooded \" bunk \" and wet blankets . [SEP] text should be one - sentence - [MASK] - line , with empty lines between documents . [MASK] sample text [MASK] [MASK] domain and was randomly selected from project gut ##tenberg . [SEP]\n",
      "I0709 04:27:28.537275 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 1000 27102 1000 10154 2018 103 2220 2008 103 1010 2021 103 2007 1037 3193 2000 103 1012 1037 17271 1999 2010 6644 4412 1010 1011 1011 3243 8335 2007 2010 23358 103 17727 12298 103 3372 14243 103 25178 1011 2018 27384 2094 103 2012 1018 1037 1012 1049 1012 1010 2007 1037 10361 1000 25277 1000 1998 4954 15019 1012 102 3793 2323 2022 2028 1011 6251 1011 103 1011 2240 1010 2007 4064 3210 2090 5491 1012 103 7099 3793 103 103 5884 1998 2001 18154 3479 2013 2622 9535 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.537819 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.538333 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.538826 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 2 6 9 12 17 22 33 36 39 40 45 71 81 84 85 0 0 0 0 0\n",
      "I0709 04:27:28.539436 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 16220 13763 2851 2025 5456 2010 1010 5178 1010 1011 2032 2566 2023 2003 2270 0 0 0 0 0\n",
      "I0709 04:27:28.540637 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.541114 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.542497 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.542951 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] this sample text is public domain and endeavour randomly selected from project gut ##tenberg . [SEP] mr [MASK] cass ##ius crossed the highway , and stopped suddenly . [SEP]\n",
      "I0709 04:27:28.543457 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2023 7099 3793 2003 2270 5884 1998 26911 18154 3479 2013 2622 9535 21806 1012 102 2720 103 16220 4173 4625 1996 3307 1010 1998 3030 3402 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.543926 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.544429 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.544871 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 8 10 14 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.545317 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2001 3479 21806 1012 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.545766 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.546194 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.547073 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.547495 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] this was nearly opposite . [SEP] at last dunes reached the quay at [MASK] opposite end of [MASK] street [MASK] and there burst on [MASK] ##am ##mon [MASK] s [MASK] eyes a vast semi [MASK] ##rcle of blue sea , ring ##ed with palaces and towers . [MASK] stopped in ##vo ##lun ##tar [MASK] ; and his little guide [MASK] also , and looked ask ##ance at the young monk , [MASK] watch the effect which that [MASK] panorama should produce on him . [SEP]\n",
      "I0709 04:27:28.547950 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2023 2001 3053 4500 1012 102 2012 2197 17746 2584 1996 21048 2012 103 4500 2203 1997 103 2395 103 1998 2045 6532 2006 103 3286 8202 103 1055 103 2159 1037 6565 4100 103 21769 1997 2630 2712 1010 3614 2098 2007 22763 1998 7626 1012 103 3030 1999 6767 26896 7559 103 1025 1998 2010 2210 5009 103 2036 1010 1998 2246 3198 6651 2012 1996 2402 8284 1010 103 3422 1996 3466 2029 2008 103 23652 2323 3965 2006 2032 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.548387 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.548821 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.549216 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 9 14 18 20 25 28 30 35 48 54 60 72 78 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.549625 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2027 1996 1996 1025 6316 1005 22741 6895 2002 6588 3030 2000 2882 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.550070 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.550475 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.551311 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.551793 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] [MASK] most of his fellow gold - [MASK] , cass was super ##sti ##tious . [SEP] while at the end of the vista in front [MASK] them gleamed the [MASK] harbour [MASK] through a network of beetles mast ##s . at [MASK] they reached the quay at the opposite end of the street ; and [MASK] burst [MASK] phil ##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea ##ban [MASK] ##ed with palaces and towers . he stopped [MASK] ##vo [MASK] liege ##ily ; and his little guide stopped [MASK] , and looked ask ##ance at the young monk , to [MASK] the effect which that grand panorama should produce on [MASK] whoa [SEP]\n",
      "I0709 04:27:28.552256 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 103 2087 1997 2010 3507 2751 1011 103 1010 16220 2001 3565 16643 20771 1012 102 2096 2012 1996 2203 1997 1996 13005 1999 2392 103 2068 25224 1996 103 7440 103 2083 1037 2897 1997 14538 15429 2015 1012 2012 103 2027 2584 1996 21048 2012 1996 4500 2203 1997 1996 2395 1025 1998 103 6532 103 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 8193 103 2098 2007 22763 1998 7626 1012 2002 3030 103 6767 103 17766 6588 1025 1998 2010 2210 5009 3030 103 1010 1998 2246 3198 6651 2012 1996 2402 8284 1010 2000 103 1996 3466 2029 2008 2882 23652 2323 3965 2006 103 23281 102 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.552702 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.553130 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.553590 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 1 8 26 30 32 37 42 56 58 74 75 84 86 87 95 107 117 118 0 0\n",
      "I0709 04:27:28.553992 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2066 24071 1997 2630 1010 14518 2197 2045 2006 1010 3614 1999 26896 7559 2036 3422 2032 1012 0 0\n",
      "I0709 04:27:28.554396 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0709 04:27:28.554785 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.555666 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.556511 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] wood pile refused to kind ##le a fire to dry his bed [MASK] clothes , and he had [MASK] ##ours ##e [MASK] a more provide ##nt [MASK] ' s to supply the deficiency . this was nearly opposite . mr . cass ##ius crossed the highway , and stopped [MASK] . something [MASK] ##ed in the nearest [MASK] pool before him . gold [SEP] wonderful to relate , not [MASK] irregular , shape ##less fragment of crude ore , fresh [MASK] nature ' [MASK] cr ##ucible , but a bit of [MASK] ##er ' s hand ##ic ##raf ##t in [MASK] form of a [MASK] gold ring . [MASK] at it more at ##ten ##tively [MASK] [MASK] saw that it bore the inscription , \" may [SEP]\n",
      "I0709 04:27:28.556968 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 3536 8632 4188 2000 2785 2571 1037 2543 2000 4318 2010 2793 103 4253 1010 1998 2002 2018 103 22957 2063 103 1037 2062 3073 3372 103 1005 1055 2000 4425 1996 18888 1012 2023 2001 3053 4500 1012 2720 1012 16220 4173 4625 1996 3307 1010 1998 3030 103 1012 2242 103 2098 1999 1996 7205 103 4770 2077 2032 1012 2751 102 6919 2000 14396 1010 2025 103 12052 1010 4338 3238 15778 1997 13587 10848 1010 4840 103 3267 1005 103 13675 21104 1010 2021 1037 2978 1997 103 2121 1005 1055 2192 2594 27528 2102 1999 103 2433 1997 1037 103 2751 3614 1012 103 2012 2009 2062 2012 6528 25499 103 103 2387 2008 2009 8501 1996 9315 1010 1000 2089 102\n",
      "I0709 04:27:28.557446 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.557873 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.558266 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 1 13 19 21 22 27 50 53 58 70 81 84 92 101 105 109 116 117 120 0\n",
      "I0709 04:27:28.558665 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 3536 1011 28667 2063 2000 11429 3402 27566 2417 2019 2013 1055 13713 1996 5810 2559 1010 2002 2009 0\n",
      "I0709 04:27:28.559061 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.559442 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.563246 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.563784 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] ! but , wonderful to relate , not an irregular , [MASK] ##less fragment of [MASK] [MASK] , fresh from nature ' s cr [MASK] , but a [MASK] of jewel ##er ' s hand ##ic ##raf ##t in the form of a plain gold [MASK] . looking at it [MASK] [MASK] [MASK] ##tively , he saw that [MASK] bore the inscription , [SEP] [MASK] on [MASK] more than a mile up the great main [MASK] , crossed in the centre of the city [MASK] at right angles , by one equally magnificent , at each end of which , miles away , [MASK] , dim and distant over [MASK] heads of the living stream golden passengers , the yellow [MASK] - hills of the desert [SEP]\n",
      "I0709 04:27:28.564266 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 999 2021 1010 6919 2000 14396 1010 2025 2019 12052 1010 103 3238 15778 1997 103 103 1010 4840 2013 3267 1005 1055 13675 103 1010 2021 1037 103 1997 13713 2121 1005 1055 2192 2594 27528 2102 1999 1996 2433 1997 1037 5810 2751 103 1012 2559 2012 2009 103 103 103 25499 1010 2002 2387 2008 103 8501 1996 9315 1010 102 103 2006 103 2062 2084 1037 3542 2039 1996 2307 2364 103 1010 4625 1999 1996 2803 1997 1996 2103 103 2012 2157 12113 1010 2011 2028 8053 12047 1010 2012 2169 2203 1997 2029 1010 2661 2185 1010 103 1010 11737 1998 6802 2058 103 4641 1997 1996 2542 5460 3585 5467 1010 1996 3756 103 1011 4564 1997 1996 5532 102\n",
      "I0709 04:27:28.564710 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.565138 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.565602 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 7 12 16 17 25 29 46 51 52 53 59 65 67 76 85 104 110 116 121 0\n",
      "I0709 04:27:28.565984 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 1010 4338 13587 10848 21104 2978 3614 2062 2012 6528 2009 2218 2005 2395 1010 2596 1996 1997 5472 0\n",
      "I0709 04:27:28.566424 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.566829 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.567674 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.569529 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] the rain had only ceased with the gray streaks of morning at blazing star , and the [MASK] awoke to a moral [MASK] of [MASK] ##liness , and the finding of forgotten knives [MASK] tin [MASK] , and smaller camp ut ##ens ##ils , where the [MASK] showers had washed away the debris and dust [MASK] ##s before the [MASK] doors . [SEP] this sample ##sily is public domain and was randomly selected [MASK] project gut ##tenberg . [SEP]\n",
      "I0709 04:27:28.570424 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 1996 4542 2018 2069 7024 2007 1996 3897 21295 1997 2851 2012 17162 2732 1010 1998 1996 103 19179 2000 1037 7191 103 1997 103 20942 1010 1998 1996 4531 1997 6404 13227 103 9543 103 1010 1998 3760 3409 21183 6132 12146 1010 2073 1996 103 23442 2018 8871 2185 1996 11385 1998 6497 103 2015 2077 1996 103 4303 1012 102 2023 7099 26863 2003 2270 5884 1998 2001 18154 3479 103 2622 9535 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.571090 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.571623 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.572104 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 8 18 23 25 34 36 47 56 60 66 74 78 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.572545 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 3897 4093 3168 4550 1010 10268 3082 16721 6644 3793 2013 1012 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.573014 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.573473 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.575213 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.575696 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] at last they reached the [MASK] at taxi opposite end of the street ; [SEP] text should be one - sentence - per - line , with empty lines between documents . this sample text is public domain and [MASK] randomly selected [MASK] project [MASK] ##tenberg . [SEP]\n",
      "I0709 04:27:28.576203 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2012 2197 2027 2584 1996 103 2012 10095 4500 2203 1997 1996 2395 1025 102 3793 2323 2022 2028 1011 6251 1011 2566 1011 2240 1010 2007 4064 3210 2090 5491 1012 2023 7099 3793 2003 2270 5884 1998 103 18154 3479 103 2622 103 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.576701 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.577196 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.577687 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 6 8 13 38 40 43 45 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.578150 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 21048 1996 2395 5884 2001 2013 9535 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.578607 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.579053 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.579963 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.580462 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] smiled upon him ; but it was too late to [MASK] back now . his guide [MASK] [MASK] [MASK] more than a mile up ##itarian great main street , proportional in the [MASK] of the city , at right angles [MASK] by [MASK] equally magnificent , at each end of which , miles away , appeared , dim and distant over [MASK] heads of [MASK] [MASK] stream of passengers , the yellow [MASK] - hills of the desert ; while [SEP] possibly this may [MASK] been the reason why early rise ##rs in [MASK] locality , during the rainy season , adopted a thoughtful habit of body , and seldom lifted their eyes to [MASK] [MASK] ##ed or india - ink washed skies above them . [SEP]\n",
      "I0709 04:27:28.580987 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 3281 2588 2032 1025 2021 2009 2001 2205 2397 2000 103 2067 2085 1012 2010 5009 103 103 103 2062 2084 1037 3542 2039 25691 2307 2364 2395 1010 14267 1999 1996 103 1997 1996 2103 1010 2012 2157 12113 103 2011 103 8053 12047 1010 2012 2169 2203 1997 2029 1010 2661 2185 1010 2596 1010 11737 1998 6802 2058 103 4641 1997 103 103 5460 1997 5467 1010 1996 3756 103 1011 4564 1997 1996 5532 1025 2096 102 4298 2023 2089 103 2042 1996 3114 2339 2220 4125 2869 1999 103 10246 1010 2076 1996 16373 2161 1010 4233 1037 16465 10427 1997 2303 1010 1998 15839 4196 2037 2159 2000 103 103 2098 2030 2634 1011 10710 8871 15717 2682 2068 1012 102\n",
      "I0709 04:27:28.581526 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.582030 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.582451 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 11 17 18 19 25 30 33 41 43 51 62 65 66 73 85 94 112 115 116 0\n",
      "I0709 04:27:28.582895 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2735 2218 2006 2005 1996 4625 2803 1010 2028 2029 1996 1996 2542 5472 2031 2008 2037 1996 16931 0\n",
      "I0709 04:27:28.583360 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.583796 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.584663 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.585253 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] indeed , it was recorded in [MASK] star that a fortunate early [MASK] ##r had once picked up on the highway a solid chunk [MASK] gold quartz which the [MASK] had freed from its inc [MASK] ##ing soil , and washed into immediate and [MASK] popularity . [SEP] rainy season , [MASK] insult show habit of body , and seldom lifted their eyes to the rift ##ed [MASK] india - ink washed skies [MASK] them . \" cass \" beard [MASK] elliot early that morning , but not with a view to [MASK] . a leak in his [MASK] roof , - - quite [MASK] with his careless , imp ##rov ##ide ##nt habits , - - had rouse ##d him at 4 a [MASK] m [SEP]\n",
      "I0709 04:27:28.585783 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 5262 1010 2009 2001 2680 1999 103 2732 2008 1037 19590 2220 103 2099 2018 2320 3856 2039 2006 1996 3307 1037 5024 20000 103 2751 20971 2029 1996 103 2018 10650 2013 2049 4297 103 2075 5800 1010 1998 8871 2046 6234 1998 103 6217 1012 102 16373 2161 1010 103 15301 2265 10427 1997 2303 1010 1998 15839 4196 2037 2159 2000 1996 16931 2098 103 2634 1011 10710 8871 15717 103 2068 1012 1000 16220 1000 10154 103 11759 2220 2008 2851 1010 2021 2025 2007 1037 3193 2000 103 1012 1037 17271 1999 2010 103 4412 1010 1011 1011 3243 103 2007 2010 23358 1010 17727 12298 5178 3372 14243 1010 1011 1011 2018 27384 2094 2032 2012 1018 1037 103 1049 102\n",
      "I0709 04:27:28.586245 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.586726 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.587153 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 7 12 13 25 30 36 45 52 53 54 68 74 81 82 93 99 103 105 125 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.587591 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 17162 2220 4125 1997 4542 29440 20332 4233 1037 16465 2030 2682 2018 13763 5456 6644 1011 8335 1012 0\n",
      "I0709 04:27:28.588022 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.588432 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.591775 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.592231 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] this text is included to make sure unicode is handled bracelet : 力 ##加 [MASK] ##北 ##区 ##ᴵ ##ᴺ ##ᵀ ##ᵃ ##ছ [MASK] ##ট ##ড ##ণ ##ত [SEP] text should be one - [MASK] - per [MASK] line , with empty lines [MASK] documents . [SEP]\n",
      "I0709 04:27:28.592725 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2023 3793 2003 2443 2000 2191 2469 27260 2003 8971 19688 1024 1778 30305 103 30307 30308 30029 30030 30031 30032 29893 103 29895 29896 29897 29898 102 3793 2323 2022 2028 1011 103 1011 2566 103 2240 1010 2007 4064 3210 103 5491 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.595118 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.595637 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.596109 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 11 15 23 34 37 38 43 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.596541 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 7919 30306 29894 6251 1011 2240 2090 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.597002 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.597511 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.599246 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.599737 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] and there burst on phil ##am ##mon ' s astonished eyes a [MASK] semi ##ci ##rcle [MASK] blue [MASK] , ring ##ed with palaces and [MASK] . [SEP] he stopped in υ ##lun ##tar [MASK] ; and his little guide stopped also , and looked vuelta ##ance at the young monk , to watch [MASK] [MASK] which that grand panorama should produce on him . [SEP]\n",
      "I0709 04:27:28.600256 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 103 4100 6895 21769 103 2630 103 1010 3614 2098 2007 22763 1998 103 1012 102 2002 3030 1999 1175 26896 7559 103 1025 1998 2010 2210 5009 3030 2036 1010 1998 2246 21441 6651 2012 1996 2402 8284 1010 2000 3422 103 103 2029 2008 2882 23652 2323 3965 2006 2032 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.600726 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.601217 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.601676 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 3 13 17 19 26 32 35 46 55 56 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.602135 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 6532 6565 1997 2712 7626 6767 6588 3198 1996 3466 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.602561 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.603001 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.604696 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.605167 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] mr . cass ##ius crossed the highway [MASK] and stopped suddenly . something glitter ##ed [MASK] [MASK] nearest red pool before him . [SEP] ##am ##mon followed [MASK] half contempt ##uous , half wondering [MASK] what this philosophy might be , which could feed the self - con [MASK] ##it of anything so ab ##ject as his [MASK] [MASK] api ##sh guide ; but the novel roar and w ##hir [MASK] of the street , the perpetual stream of busy faces , the line of cu [MASK] ##cles , pal ##an ##quin ##s , laden [MASK] ##es , camel [MASK] , elephants , which met and passed him , and squeezed him up [MASK] and into [MASK] ##s , as they threaded their way through the [SEP]\n",
      "I0709 04:27:28.605711 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2720 1012 16220 4173 4625 1996 3307 103 1998 3030 3402 1012 2242 27566 2098 103 103 7205 2417 4770 2077 2032 1012 102 3286 8202 2628 103 2431 17152 8918 1010 2431 6603 103 2054 2023 4695 2453 2022 1010 2029 2071 5438 1996 2969 1011 9530 103 4183 1997 2505 2061 11113 20614 2004 2010 103 103 17928 4095 5009 1025 2021 1996 3117 11950 1998 1059 11961 103 1997 1996 2395 1010 1996 18870 5460 1997 5697 5344 1010 1996 2240 1997 12731 103 18954 1010 14412 2319 12519 2015 1010 14887 103 2229 1010 19130 103 1010 16825 1010 2029 2777 1998 2979 2032 1010 1998 7757 2032 2039 103 1998 2046 103 2015 1010 2004 2027 26583 2037 2126 2083 1996 102\n",
      "I0709 04:27:28.606184 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.606674 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.607097 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 8 16 17 28 35 49 58 59 60 62 66 71 87 96 100 114 117 118 125 0\n",
      "I0709 04:27:28.607538 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 1010 1999 1996 1010 2012 3401 14202 2210 17928 5009 3117 2140 18752 4632 2015 4084 7086 2015 2083 0\n",
      "I0709 04:27:28.607972 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.608395 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.609195 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.609704 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] [MASK] ##am ##mon followed , half contempt ##uous , half wondering at what [MASK] philosophy might be , which could feed the self - con ##ce ##it of anything so ab ##ject as his [MASK] little api ##sh guide ; [SEP] stream of busy faces [MASK] [MASK] line of cu ##rri ##cles , pal [MASK] ##quin ##s , laden ass ##es [MASK] camel ##s , [MASK] , which met [MASK] passed him , and [MASK] him sliding steps and into doorway ##s , as they [MASK] their way through the great moon - gate into the ample street beyond [MASK] drove everything from his mind [MASK] wondering curiosity , and a vague , helpless dread of that [MASK] [MASK] wilderness , more [MASK] than any dead [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.610200 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 103 3286 8202 2628 1010 2431 17152 8918 1010 2431 6603 2012 2054 103 4695 2453 2022 1010 2029 2071 5438 1996 2969 1011 9530 3401 4183 1997 2505 2061 11113 20614 2004 2010 103 2210 17928 4095 5009 1025 102 5460 1997 5697 5344 103 103 2240 1997 12731 18752 18954 1010 14412 103 12519 2015 1010 14887 4632 2229 103 19130 2015 1010 103 1010 2029 2777 103 2979 2032 1010 1998 103 2032 8058 4084 1998 2046 7086 2015 1010 2004 2027 103 2037 2126 2083 1996 2307 4231 1011 4796 2046 1996 20851 2395 3458 103 5225 2673 2013 2010 2568 103 6603 10628 1010 1998 1037 13727 1010 13346 14436 1997 2008 103 103 9917 1010 2062 103 2084 2151 2757 102\n",
      "I0709 04:27:28.610665 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.611158 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.611586 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 1 14 24 35 38 46 47 55 62 66 70 75 77 86 100 106 118 119 123 0\n",
      "I0709 04:27:28.612028 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 6316 2023 1011 14202 4095 1010 1996 2319 1010 16825 1998 7757 2039 26583 1010 2021 2307 2542 6659 0\n",
      "I0709 04:27:28.612466 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.612874 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.615410 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.615950 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] possibly this may have been the reason why early rise ##rs in that [MASK] , during the rainy season , adopted [MASK] thoughtful habit of body , and seldom lifted their eyes to the rift ##ed or india ##riety ink washed [MASK] above them . \" cass \" beard had [MASK] early that morning , but not with a view to discovery ##ticus [SEP] consistent with his [MASK] , imp ##ret [MASK] ##nt habits , - - had rouse ##d him at 4 [MASK] . m . , with a [MASK] \" bunk \" and [MASK] blankets . the chips from [MASK] wood pile refused to [MASK] ##le [MASK] fire to dry his bed [MASK] clothes , and he twentieth rec ##ours ##e to a more [SEP]\n",
      "I0709 04:27:28.616571 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 4298 2023 2089 2031 2042 1996 3114 2339 2220 4125 2869 1999 2008 103 1010 2076 1996 16373 2161 1010 4233 103 16465 10427 1997 2303 1010 1998 15839 4196 2037 2159 2000 1996 16931 2098 2030 2634 27840 10710 8871 103 2682 2068 1012 1000 16220 1000 10154 2018 103 2220 2008 2851 1010 2021 2025 2007 1037 3193 2000 5456 29587 102 8335 2007 2010 103 1010 17727 13465 103 3372 14243 1010 1011 1011 2018 27384 2094 2032 2012 1018 103 1012 1049 1012 1010 2007 1037 103 1000 25277 1000 1998 103 15019 1012 1996 11772 2013 103 3536 8632 4188 2000 103 2571 103 2543 2000 4318 2010 2793 103 4253 1010 1998 2002 9086 28667 22957 2063 2000 1037 2062 102\n",
      "I0709 04:27:28.617073 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.617630 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.618093 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 14 22 39 42 51 63 68 71 72 84 90 91 96 102 107 109 115 116 120 0\n",
      "I0709 04:27:28.618545 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 10246 1037 1011 15717 13763 1012 23358 12298 5178 1037 1037 10361 4954 2010 2785 1037 1011 4253 2018 0\n",
      "I0709 04:27:28.618987 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.619441 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.621211 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.621726 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] the rain had only ceased with [MASK] [MASK] streaks of morning at blazing [MASK] , and the settlement awoke to a moral sense of clean ##liness , and the finding of forgotten knives , tin cups , and smaller camp ut ##ens ##ils , where the heavy showers [MASK] washed away the debris and dust [MASK] ##s before the cabin [MASK] . [SEP] ##r had once picked up on the highway a solid chunk [MASK] gold quartz which the rain [MASK] freed from its inc ##umber ##ing soil , and [MASK] into immediate and [MASK] popularity . possibly this may have been the reason why early rise ##rs [MASK] that locality [MASK] during [MASK] [MASK] [MASK] , adopted a thoughtful habit of body , and seldom [SEP]\n",
      "I0709 04:27:28.622242 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 1996 4542 2018 2069 7024 2007 103 103 21295 1997 2851 2012 17162 103 1010 1998 1996 4093 19179 2000 1037 7191 3168 1997 4550 20942 1010 1998 1996 4531 1997 6404 13227 1010 9543 10268 1010 1998 3760 3409 21183 6132 12146 1010 2073 1996 3082 23442 103 8871 2185 1996 11385 1998 6497 103 2015 2077 1996 6644 103 1012 102 2099 2018 2320 3856 2039 2006 1996 3307 1037 5024 20000 103 2751 20971 2029 1996 4542 103 10650 2013 2049 4297 29440 2075 5800 1010 1998 103 2046 6234 1998 103 6217 1012 4298 2023 2089 2031 2042 1996 3114 2339 2220 4125 2869 103 2008 10246 103 2076 103 103 103 1010 4233 1037 16465 10427 1997 2303 1010 1998 15839 102\n",
      "I0709 04:27:28.622738 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.623241 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0709 04:27:28.623711 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 7 8 14 24 45 49 56 61 62 75 77 81 91 95 109 112 114 115 116 0\n",
      "I0709 04:27:28.624146 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 1996 3897 2732 1997 2073 2018 16721 4303 1012 1997 20971 2018 8871 20332 1999 1010 1996 16373 2161 0\n",
      "I0709 04:27:28.624581 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "I0709 04:27:28.625017 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.625867 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.626331 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] but , wonderful to relate , not an irregular , [MASK] outer fragment of crude [MASK] , fresh [MASK] nature ' s cr ##ucible [MASK] but a bit of [MASK] [MASK] ' s hand ##ic ##raf ##t in the form [MASK] a plain gold ring . [SEP] looking at it [MASK] at ##ten ##tively , he saw that worries bore the [MASK] , \" may to cass . \" like most of [MASK] [MASK] gold - seekers , cass was super ##sti ##tious . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.626818 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2021 1010 6919 2000 14396 1010 2025 2019 12052 1010 103 6058 15778 1997 13587 103 1010 4840 103 3267 1005 1055 13675 21104 103 2021 1037 2978 1997 103 103 1005 1055 2192 2594 27528 2102 1999 1996 2433 103 1037 5810 2751 3614 1012 102 2559 2012 2009 103 2012 6528 25499 1010 2002 2387 2008 15508 8501 1996 103 1010 1000 2089 2000 16220 1012 1000 2066 2087 1997 103 103 2751 1011 24071 1010 16220 2001 3565 16643 20771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.627289 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.627764 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.628194 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 11 12 16 19 25 30 31 41 51 59 62 73 74 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.628612 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 4338 3238 10848 2013 1010 13713 2121 1997 2062 2009 9315 2010 3507 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.629057 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.629499 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.630311 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.630765 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] there [MASK] a phil vita ##phic pleasure in [MASK] one ' [MASK] treasures to the modest [MASK] . [SEP] looking [MASK] it more at ##ten ##tively , he saw that it [MASK] the inscription , \" may to cass . \" like most of his fellow gold [MASK] seekers [MASK] cass was super ##sti ##tious . [SEP]\n",
      "I0709 04:27:28.631244 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 2045 103 1037 6316 19300 17926 5165 1999 103 2028 1005 103 17605 2000 1996 10754 103 1012 102 2559 103 2009 2062 2012 6528 25499 1010 2002 2387 2008 2009 103 1996 9315 1010 1000 2089 2000 16220 1012 1000 2066 2087 1997 2010 3507 2751 103 24071 103 16220 2001 3565 16643 20771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.631703 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.632172 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.632621 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 2 5 9 12 17 21 32 48 50 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.633094 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2003 19137 3098 1055 2402 2012 8501 1011 1010 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.633580 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.634020 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 1\n",
      "I0709 04:27:28.637306 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.637861 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] [MASK] glitter ##ed in the nearest red pool before him . gold , surely ! [SEP] but , wonderful to relate , not an irregular , shape [MASK] fragment of crude [MASK] [MASK] fresh from ##ze ' s cr ##ucible , but a bit of jewel ##er ' s hand ##ic ##raf [MASK] in the ##zzling of a plain [MASK] ring . looking at it more at ##ten ##tively [MASK] [MASK] saw that it bore the inscription [MASK] \" may to cass . \" like most of his [MASK] gold - [MASK] , cass was super ##sti ##tious . [SEP]\n",
      "I0709 04:27:28.638360 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 103 27566 2098 1999 1996 7205 2417 4770 2077 2032 1012 2751 1010 7543 999 102 2021 1010 6919 2000 14396 1010 2025 2019 12052 1010 4338 103 15778 1997 13587 103 103 4840 2013 4371 1005 1055 13675 21104 1010 2021 1037 2978 1997 13713 2121 1005 1055 2192 2594 27528 103 1999 1996 20838 1997 1037 5810 103 3614 1012 2559 2012 2009 2062 2012 6528 25499 103 103 2387 2008 2009 8501 1996 9315 103 1000 2089 2000 16220 1012 1000 2066 2087 1997 2010 103 2751 1011 103 1010 16220 2001 3565 16643 20771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.638838 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.639308 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.639733 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 1 28 32 33 36 39 53 56 60 66 70 71 78 89 92 0 0 0 0 0\n",
      "I0709 04:27:28.640175 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2242 3238 10848 1010 3267 13675 2102 2433 2751 2062 1010 2002 1010 3507 24071 0 0 0 0 0\n",
      "I0709 04:27:28.640610 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.641018 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.641901 140445336774400 <ipython-input-68-17008c885911>:54] *** Example ***\n",
      "I0709 04:27:28.642361 140445336774400 <ipython-input-68-17008c885911>:56] tokens: [CLS] text should be [MASK] - sentence - per - line , with empty lines between documents [MASK] [SEP] this sample text is [MASK] domain and was [MASK] selected from [MASK] gut ##tenberg . [SEP]\n",
      "I0709 04:27:28.642834 140445336774400 <ipython-input-68-17008c885911>:66] input_ids: 101 3793 2323 2022 103 1011 6251 1011 2566 1011 2240 1010 2007 4064 3210 2090 5491 103 102 2023 7099 3793 2003 103 5884 1998 2001 103 3479 2013 103 9535 21806 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.643290 140445336774400 <ipython-input-68-17008c885911>:66] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.643772 140445336774400 <ipython-input-68-17008c885911>:66] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.644192 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_positions: 4 17 23 27 30 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 04:27:28.644634 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_ids: 2028 1012 2270 18154 2622 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0709 04:27:28.645068 140445336774400 <ipython-input-68-17008c885911>:66] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "I0709 04:27:28.645512 140445336774400 <ipython-input-68-17008c885911>:66] next_sentence_labels: 0\n",
      "I0709 04:27:28.674439 140445336774400 <ipython-input-68-17008c885911>:71] Wrote 118 total instances\n"
     ]
    }
   ],
   "source": [
    "write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                  max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "\n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "  return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like most people I was intrigued when I heard ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The original \"les visiteurs\" was original, hil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just watched the film for the 3rd time and enj...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the only scenes wich made me laugh where the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  Like most people I was intrigued when I heard ...         1         0\n",
       "1  It's a strange feeling to sit alone in a theat...         8         1\n",
       "2  The original \"les visiteurs\" was original, hil...         1         0\n",
       "3  Just watched the film for the 3rd time and enj...        10         1\n",
       "4  the only scenes wich made me laugh where the o...         1         0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_df, test_df = download_and_load_datasets()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = train_df['polarity'].tolist()\n",
    "\n",
    "test_text = test_df['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = test_df['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2cb5245f87495fa3bef23da3e25c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9341d95fcc464075ba02b3cbe4c27f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Old horror movies are interesting, plenty of s...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bette Midler showcases her talents and beauty ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roeg has done some great movies, but this a tu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mel Gibson's Braveheart was a spectacularly ac...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one military drama I like a lot! Tom B...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  Old horror movies are interesting, plenty of s...        10         1\n",
       "1  Bette Midler showcases her talents and beauty ...        10         1\n",
       "2  Roeg has done some great movies, but this a tu...         1         0\n",
       "3  Mel Gibson's Braveheart was a spectacularly ac...         9         1\n",
       "4  This is one military drama I like a lot! Tom B...         9         1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fine_tune_layers=10\n",
    "pooling=\"first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hub.Module(\n",
    "            bert_path, trainable=True, name=f\"bert_module\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bert_module_1/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_vars=bert.variables\n",
    "trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bert_module_1/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module_1/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "trainable_layers = [\"pooler/dense\"]\n",
    "trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bert_module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>,\n",
       " <tf.Variable 'bert_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred\n",
    "                                 \n",
    "                                 \n",
    "                                 \n",
    "                                 \n",
    "                                 \n",
    "                                 \n",
    "                                 )\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_3 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      bert_layer_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 22,051,329\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "25000/25000 [==============================] - 277s 11ms/sample - loss: 0.7309 - acc: 0.5008 - val_loss: 0.6945 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5de4bed30>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
