{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled30.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurvaChiniya/Sentiment_analysis/blob/master/SELF_ATT_TEXT_CLASSIFIER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4dvmkvnpF_",
        "colab_type": "text"
      },
      "source": [
        "# IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9OgZoKde8lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchtext.vocab import Vectors, GloVe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lRs2DGDn9NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize = lambda x: x.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLVFJh45en6",
        "colab_type": "text"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "184KJsbXsr3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "LABEL = data.LabelField()\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsqA5dU4sudg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c45d60af-7f96-4f4c-99c7-a80f48f8e454"
      },
      "source": [
        "word_embeddings = TEXT.vocab.vectors\n",
        "len(word_embeddings)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "251639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lydqYgEs2-n",
        "colab_type": "code",
        "outputId": "c5a05d49-4dd0-497b-ae6f-43ec557bbf7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "print (\"Label Length: \" + str(len(LABEL.vocab)))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 251639\n",
            "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
            "Label Length: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMAmN5A_tD0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split()\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLEBa5crBCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKY9GKtjO9Sq",
        "colab_type": "text"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQdEt6qEDJaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\tself.weights = weights\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\t\tself.label = nn.Linear(2000, output_size)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\tlogits = self.label(fc_out)\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJnvMaLeO4GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUQvb9mwPgou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzMvFfGjPna7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYYviE_tO4fd",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1NWR6Ek3rq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0b1187ec-0a6b-4563-e56d-2d7ae083c3c9"
      },
      "source": [
        "\n",
        "\n",
        "learning_rate = 2e-5\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "in_channels=1\n",
        "kernel_heights=[3,4,5]\n",
        "stride=1\n",
        "padding=1\n",
        "out_channels=1\n",
        "keep_probab=0.5\n",
        "model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, weights)\n",
        "\t\t\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = F.cross_entropy\n",
        "\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqHmZZFCTa6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2221c386-0db9-4659-f23f-3a7b513d5f79"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "251639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUSH9mA4OdBB",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnKi_M7EtxtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "407f4e1f-0f4a-4e57-e32d-ef7e1befe9ba"
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "\n"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Idx: 100, Training Loss: 0.7657, Training Accuracy:  65.62%\n",
            "Epoch: 1, Idx: 200, Training Loss: 0.4644, Training Accuracy:  75.00%\n",
            "Epoch: 1, Idx: 300, Training Loss: 0.3323, Training Accuracy:  84.38%\n",
            "Epoch: 1, Idx: 400, Training Loss: 0.3756, Training Accuracy:  87.50%\n",
            "Epoch: 1, Idx: 500, Training Loss: 0.3680, Training Accuracy:  81.25%\n",
            "Epoch: 01, Train Loss: 0.536, Train Acc: 74.78%, Val. Loss: 0.384155, Val. Acc: 81.94%\n",
            "Epoch: 2, Idx: 100, Training Loss: 0.0666, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 200, Training Loss: 0.2663, Training Accuracy:  87.50%\n",
            "Epoch: 2, Idx: 300, Training Loss: 0.0962, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 400, Training Loss: 0.1714, Training Accuracy:  93.75%\n",
            "Epoch: 2, Idx: 500, Training Loss: 0.1550, Training Accuracy:  90.62%\n",
            "Epoch: 02, Train Loss: 0.254, Train Acc: 89.86%, Val. Loss: 0.395648, Val. Acc: 83.47%\n",
            "Epoch: 3, Idx: 100, Training Loss: 0.0440, Training Accuracy:  96.88%\n",
            "Epoch: 3, Idx: 200, Training Loss: 0.1914, Training Accuracy:  93.75%\n",
            "Epoch: 3, Idx: 300, Training Loss: 0.0653, Training Accuracy:  96.88%\n",
            "Epoch: 3, Idx: 400, Training Loss: 0.0109, Training Accuracy:  100.00%\n",
            "Epoch: 3, Idx: 500, Training Loss: 0.0643, Training Accuracy:  96.88%\n",
            "Epoch: 03, Train Loss: 0.114, Train Acc: 95.84%, Val. Loss: 0.586999, Val. Acc: 83.40%\n",
            "Epoch: 4, Idx: 100, Training Loss: 0.0490, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 200, Training Loss: 0.3334, Training Accuracy:  90.62%\n",
            "Epoch: 4, Idx: 300, Training Loss: 0.0598, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 400, Training Loss: 0.2166, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 500, Training Loss: 0.0403, Training Accuracy:  96.88%\n",
            "Epoch: 04, Train Loss: 0.042, Train Acc: 98.60%, Val. Loss: 0.912384, Val. Acc: 82.74%\n",
            "Epoch: 5, Idx: 100, Training Loss: 0.0068, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 200, Training Loss: 0.0013, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 300, Training Loss: 0.2413, Training Accuracy:  96.88%\n",
            "Epoch: 5, Idx: 400, Training Loss: 0.0017, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 500, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 05, Train Loss: 0.027, Train Acc: 99.09%, Val. Loss: 1.312786, Val. Acc: 82.45%\n",
            "Epoch: 6, Idx: 100, Training Loss: 0.1334, Training Accuracy:  96.88%\n",
            "Epoch: 6, Idx: 200, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 300, Training Loss: 0.4200, Training Accuracy:  96.88%\n",
            "Epoch: 6, Idx: 400, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 500, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 06, Train Loss: 0.034, Train Acc: 99.20%, Val. Loss: 1.534689, Val. Acc: 82.49%\n",
            "Epoch: 7, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 200, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 300, Training Loss: 0.0649, Training Accuracy:  96.88%\n",
            "Epoch: 7, Idx: 400, Training Loss: 0.0417, Training Accuracy:  96.88%\n",
            "Epoch: 7, Idx: 500, Training Loss: 0.0031, Training Accuracy:  100.00%\n",
            "Epoch: 07, Train Loss: 0.028, Train Acc: 99.31%, Val. Loss: 2.777680, Val. Acc: 82.63%\n",
            "Epoch: 8, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 200, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 300, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 400, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 500, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 08, Train Loss: 0.026, Train Acc: 99.35%, Val. Loss: 2.166419, Val. Acc: 83.00%\n",
            "Epoch: 9, Idx: 100, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 200, Training Loss: 0.0112, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 300, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 400, Training Loss: 0.0005, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 500, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 09, Train Loss: 0.033, Train Acc: 99.33%, Val. Loss: 4.127054, Val. Acc: 80.01%\n",
            "Epoch: 10, Idx: 100, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 200, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 300, Training Loss: 0.0024, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 400, Training Loss: 0.1212, Training Accuracy:  96.88%\n",
            "Epoch: 10, Idx: 500, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
            "Epoch: 10, Train Loss: 0.023, Train Acc: 99.46%, Val. Loss: 3.331166, Val. Acc: 82.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVeO-gg1Ofkc",
        "colab_type": "text"
      },
      "source": [
        "# ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLrfzwzvOka1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2a56aeff-e7fe-4e53-bdd9-2f9c915f3d8a"
      },
      "source": [
        "    \n",
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.897, Test Acc: 79.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFC0Q1qgOVF6",
        "colab_type": "text"
      },
      "source": [
        "# PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDCPNalYOWds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "20ff3aaf-5c8b-4fa6-81df-24651c6a6a4a"
      },
      "source": [
        "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
        "\n",
        "test_sen1 = TEXT.preprocess(test_sen1)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "test_sen2 = TEXT.preprocess(test_sen2)\n",
        "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n",
        "test_sen = np.asarray(test_sen1)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor, 1)\n",
        "out = F.softmax(output, 1)\n",
        "if (torch.argmax(out[0]) == 1):\n",
        "    print (\"Sentiment: Positive\")\n",
        "else:\n",
        "    print (\"Sentiment: Negative\")"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Positive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2SWGtAcOXO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3c32ca48-c86a-4c37-d9d7-84fe58322a6d"
      },
      "source": [
        "test_sen = np.asarray(test_sen2)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor, 1)\n",
        "out = F.softmax(output, 1)\n",
        "if (torch.argmax(out[0]) == 1):\n",
        "    print (\"Sentiment: Positive\")\n",
        "else:\n",
        "    print (\"Sentiment: Negative\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment: Negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTXtN1-qRQPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}